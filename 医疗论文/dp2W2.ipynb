{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "import transformers as ts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset as arkData\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer,GlobalPointerBertConfig,GlobalPointerBert,get_default_model_optimizer,Task,Predictor\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner_list</th>\n",
       "      <th>type</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>血管内皮生长因子表达水平与宫颈癌患者预后的关系</td>\n",
       "      <td>[{'mention': '血管内皮生长因子表达水平与宫颈癌患者预后的关系', 'label...</td>\n",
       "      <td>Title</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>目的探讨地尔硫卓在治疗扩张型心肌病中的临床疗效。方法将2014年9月至2017年12月在红河...</td>\n",
       "      <td>[{'mention': '探讨地尔硫卓在治疗扩张型心肌病中的临床疗效', 'label':...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>地尔硫卓在治疗扩张型心肌病中的临床疗效观察</td>\n",
       "      <td>[{'mention': '地尔硫卓在治疗扩张型心肌病中的临床疗效观察', 'label':...</td>\n",
       "      <td>Title</td>\n",
       "      <td>2501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                            血管内皮生长因子表达水平与宫颈癌患者预后的关系   \n",
       "1  目的探讨地尔硫卓在治疗扩张型心肌病中的临床疗效。方法将2014年9月至2017年12月在红河...   \n",
       "2                              地尔硫卓在治疗扩张型心肌病中的临床疗效观察   \n",
       "\n",
       "                                            ner_list      type  doc_id  \n",
       "0  [{'mention': '血管内皮生长因子表达水平与宫颈癌患者预后的关系', 'label...     Title    2500  \n",
       "1  [{'mention': '探讨地尔硫卓在治疗扩张型心肌病中的临床疗效', 'label':...  Abstract    2501  \n",
       "2  [{'mention': '地尔硫卓在治疗扩张型心肌病中的临床疗效观察', 'label':...     Title    2501  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_json('./datasets/train.json')\n",
    "lens=len(df_train)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SUZIKA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.052 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#用于W2NER\n",
    "result=[]\n",
    "for i in range(0,lens):\n",
    "    if df_train.loc[i,'type'] == 'Title':\n",
    "        text=df_train.loc[i,'text'].replace(' ','')\n",
    "        #token\n",
    "        sentence=[char for char in text]\n",
    "        #实体\n",
    "        ner=[]\n",
    "        for n in df_train.loc[i,'ner_list']:\n",
    "            entity=n['mention'].replace(' ','')\n",
    "            l=text.index(entity)\n",
    "            r=l+len(entity)\n",
    "            index=[j for j in range(l,r)]\n",
    "            type=n['label'][0]\n",
    "            ner.append({\n",
    "                'index':index,\n",
    "                'type':type,\n",
    "            })\n",
    "        #分词\n",
    "        word=[]\n",
    "        split_word=jieba.tokenize(text)\n",
    "        for w in split_word:\n",
    "            widx=[j for j in range(w[1],w[2])]\n",
    "            word.append(widx)\n",
    "        #组装\n",
    "        result.append({\n",
    "            'sentence':sentence,\n",
    "            'ner':ner,\n",
    "            'word':word,\n",
    "        })\n",
    "result_json=json.dumps(result,ensure_ascii = False)\n",
    "with open('./datasets/w2ner/train.json','w',encoding='utf-8')as f:\n",
    "    f.write(result_json)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigcat=[]\n",
    "title_ner=[]\n",
    "smallcat={\n",
    "    'C':[],\n",
    "    'I':[],\n",
    "    'O':[],\n",
    "    'P':[],\n",
    "    'S':[],\n",
    "}\n",
    "for i in range(0,lens):\n",
    "    if df_train.loc[i,'type'] == 'Title':\n",
    "        newner=[]\n",
    "        text=df_train.loc[i,'text'].replace(' ','')\n",
    "        for ner in df_train.loc[i,'ner_list']:\n",
    "            l=ner['label'][0]\n",
    "            newbig={\n",
    "                'text':ner['mention'],\n",
    "                'label':l,\n",
    "            }\n",
    "            bigcat.append(newbig)\n",
    "            \n",
    "            smallcat[l].append({\n",
    "                'text':ner['mention'],\n",
    "                'label':ner['label'][2:],\n",
    "            })\n",
    "            \n",
    "            subtext=ner['mention'].replace(' ','')\n",
    "            sidx=text.find(subtext)\n",
    "            if sidx == -1:\n",
    "                continue\n",
    "            eidx=sidx+len(subtext)\n",
    "            newner.append({\n",
    "                'start_idx':sidx,\n",
    "                'end_idx':eidx,\n",
    "                'type':ner['label'][0],\n",
    "                'entity':subtext,\n",
    "            })\n",
    "            \n",
    "        title_ner.append({\n",
    "            'text':df_train.loc[i,'text'],\n",
    "            'label':newner,\n",
    "        })\n",
    "    else:\n",
    "        for ner in df_train.loc[i,'ner_list']:\n",
    "            l=ner['label'][0]\n",
    "            curtext=ner['mention']\n",
    "            if len(curtext) > 200:\n",
    "                todel=len(curtext)-198\n",
    "                ll=todel//2\n",
    "                r=todel-ll\n",
    "                curtext=curtext[ll:-r]\n",
    "            newbig={\n",
    "                'text':curtext,\n",
    "                'label':l,\n",
    "            }\n",
    "            bigcat.append(newbig)\n",
    "            smallcat[l].append({\n",
    "                'text':curtext,\n",
    "                'label':ner['label'][2:],\n",
    "            })\n",
    "big_json=json.dumps(bigcat,ensure_ascii = False)\n",
    "with open('./datasets/hierarchical/bigcat_train.json','w',encoding='utf-8') as f:\n",
    "    f.write(big_json)\n",
    "    f.close()\n",
    "    \n",
    "title_json=json.dumps(title_ner,ensure_ascii = False)\n",
    "with open('./datasets/hierarchical/titlener_train.json','w',encoding='utf-8') as f:\n",
    "    f.write(title_json)\n",
    "    f.close()\n",
    "    \n",
    "for key in smallcat.keys():\n",
    "    small_json=json.dumps(smallcat[key],ensure_ascii = False)\n",
    "    with open('./datasets/hierarchical/'+key+'cat_train.json','w',encoding='utf-8') as f:\n",
    "        f.write(small_json)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt=\"采用抽签法将安阳市中医院心病一科2014年3月~2015年4月113例不稳定型心绞痛患者进行分组,对照组56例给予疏血通注射液6mL/d治疗,观察组57例给予疏血通注射液10mL/d治疗,观察两组临床疗效及不良反应\"\n",
    "analyse.extract_tags(ttt,topK=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=ts.BertTokenizer.from_pretrained('././../../model/macbert-base-chinese-medical-collation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids=tt('乌拉地尔复合艾司洛尔控制[MASK]在高血压患者脊柱手术中的应用')['input_ids']\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner_list</th>\n",
       "      <th>type</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>常染色体显性多囊肾病的临床问题及其肾脏替代治疗的选择</td>\n",
       "      <td>[]</td>\n",
       "      <td>Title</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>[{'mention': '表明,在体外条件下,胃癌干细胞对5-氟尿嘧啶敏感性较低,推测其可...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>胃癌干细胞对5-氟尿嘧啶的敏感性</td>\n",
       "      <td>[]</td>\n",
       "      <td>Title</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text  \\\n",
       "0  常染色体显性多囊肾病的临床问题及其肾脏替代治疗的选择   \n",
       "1                               \n",
       "2            胃癌干细胞对5-氟尿嘧啶的敏感性   \n",
       "\n",
       "                                            ner_list      type  doc_id  \n",
       "0                                                 []     Title     500  \n",
       "1  [{'mention': '表明,在体外条件下,胃癌干细胞对5-氟尿嘧啶敏感性较低,推测其可...  Abstract     501  \n",
       "2                                                 []     Title     501  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_json('./datasets/test.json')\n",
    "lenst=len(df_test)\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RhoC',\n",
       " '的',\n",
       " '表达',\n",
       " '与',\n",
       " '肿瘤',\n",
       " '细胞',\n",
       " '分化',\n",
       " '、',\n",
       " '浸润',\n",
       " '深度',\n",
       " '未见',\n",
       " '相关性',\n",
       " '(',\n",
       " 'P',\n",
       " '=',\n",
       " '0.977',\n",
       " ',',\n",
       " 'P',\n",
       " '=',\n",
       " '0.141',\n",
       " ')',\n",
       " ',',\n",
       " '与',\n",
       " '淋巴结',\n",
       " '转移',\n",
       " '、',\n",
       " 'PTNM',\n",
       " '分期',\n",
       " '存在',\n",
       " '相关性',\n",
       " '(',\n",
       " 'P',\n",
       " '=',\n",
       " '0.014',\n",
       " ',',\n",
       " 'P',\n",
       " '=',\n",
       " '0.039',\n",
       " ')']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp=jieba.tokenize('RhoC的表达与肿瘤细胞分化、浸润深度未见相关性(P=0.977,P=0.141),与淋巴结转移、PTNM分期存在相关性(P=0.014,P=0.039)')\n",
    "[word[0] for word in sp ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def contains_alphanumeric(input_string):\n",
    "    if re.search(r'[a-zA-Z0-9!@#$%^&*()~≥=();:,./]', input_string):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in range(0,lens):\n",
    "    if df_train.loc[i,'type'] == 'Title':\n",
    "        text=df_train.loc[i,'text']\n",
    "        if len(text) > 200:\n",
    "                todel=len(text)-198\n",
    "                ll=todel//2\n",
    "                r=todel-ll\n",
    "                text=text[ll:-r]\n",
    "        text_words=jieba.tokenize(text)\n",
    "        text_words=[words[0] for words in text_words]\n",
    "        tlen=len(text_words)\n",
    "        if tlen < 5:\n",
    "            continue\n",
    "        nmask=max(int(tlen*0.2),1)\n",
    "        mask_idx=np.random.choice(np.arange(0,tlen),size=nmask,replace=False)\n",
    "        for idx in mask_idx:\n",
    "            if contains_alphanumeric(text_words[idx]):\n",
    "                text_words[idx]=len(text_words[idx])*'[MASK]'\n",
    "        result.append({\n",
    "                'text':text,\n",
    "                'masked':''.join(text_words)\n",
    "            })\n",
    "    if df_train.loc[i,'type'] == 'Abstract':\n",
    "        for entity in df_train.loc[i,'ner_list']:\n",
    "            text=entity['mention']\n",
    "            if len(text) > 200:\n",
    "                    todel=len(text)-198\n",
    "                    ll=todel//2\n",
    "                    r=todel-ll\n",
    "                    text=text[ll:-r]\n",
    "            text_words=jieba.tokenize(text)\n",
    "            text_words=[words[0] for words in text_words]\n",
    "            tlen=len(text_words)\n",
    "            if tlen < 5:\n",
    "                continue\n",
    "            nmask=max(int(tlen*0.2),1)\n",
    "            mask_idx=np.random.choice(np.arange(0,tlen),size=nmask,replace=False)\n",
    "            for idx in mask_idx:\n",
    "                if contains_alphanumeric(text_words[idx]):\n",
    "                    text_words[idx]=len(text_words[idx])*'[MASK]'\n",
    "            result.append({\n",
    "                    'text':text,\n",
    "                    'masked':''.join(text_words)\n",
    "                })\n",
    "for i in range(0,lenst):\n",
    "    if df_test.loc[i,'type'] == 'Title':\n",
    "        text=df_test.loc[i,'text']\n",
    "        if len(text) > 200:\n",
    "                todel=len(text)-198\n",
    "                ll=todel//2\n",
    "                r=todel-ll\n",
    "                text=text[ll:-r]\n",
    "        text_words=jieba.tokenize(text)\n",
    "        text_words=[words[0] for words in text_words]\n",
    "        tlen=len(text_words)\n",
    "        if tlen < 5:\n",
    "            continue\n",
    "        nmask=max(int(tlen*0.2),1)\n",
    "        mask_idx=np.random.choice(np.arange(0,tlen),size=nmask,replace=False)\n",
    "        for idx in mask_idx:\n",
    "            if contains_alphanumeric(text_words[idx]):\n",
    "                text_words[idx]=len(text_words[idx])*'[MASK]'\n",
    "        result.append({\n",
    "                'text':text,\n",
    "                'masked':''.join(text_words)\n",
    "            })\n",
    "    if df_test.loc[i,'type'] == 'Abstract':\n",
    "        for entity in df_test.loc[i,'ner_list']:\n",
    "            text=entity['mention']\n",
    "            if len(text) > 200:\n",
    "                    todel=len(text)-198\n",
    "                    ll=todel//2\n",
    "                    r=todel-ll\n",
    "                    text=text[ll:-r]\n",
    "            text_words=jieba.tokenize(text)\n",
    "            text_words=[words[0] for words in text_words]\n",
    "            tlen=len(text_words)\n",
    "            if tlen < 5:\n",
    "                continue\n",
    "            nmask=max(int(tlen*0.2),1)\n",
    "            mask_idx=np.random.choice(np.arange(0,tlen),size=nmask,replace=False)\n",
    "            for idx in mask_idx:\n",
    "                if contains_alphanumeric(text_words[idx]):\n",
    "                    text_words[idx]=len(text_words[idx])*'[MASK]'\n",
    "            result.append({\n",
    "                    'text':text,\n",
    "                    'masked':''.join(text_words)\n",
    "                })\n",
    "result_json=json.dumps(result,ensure_ascii = False)\n",
    "with open('./datasets/fintune.json','w',encoding='utf-8')as f:\n",
    "    f.write(result_json)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in range(0,lenst):\n",
    "    if df_train.loc[i,'type'] == 'Title':\n",
    "        result.append(df_train.loc[i,'text'])\n",
    "    if df_train.loc[i,'type'] == 'Abstract':\n",
    "        for entity in df_train.loc[i,'ner_list']:\n",
    "            result.append(entity['mention'])\n",
    "for i in range(0,lenst):\n",
    "    if df_test.loc[i,'type'] == 'Title':\n",
    "        result.append(df_test.loc[i,'text'])\n",
    "    if df_test.loc[i,'type'] == 'Abstract':\n",
    "        for entity in df_test.loc[i,'ner_list']:\n",
    "            result.append(entity['mention'])\n",
    "with open('./datasets/sentence.txt','w',encoding='utf-8') as f:\n",
    "    for r in result:\n",
    "        f.write(r+'\\n')\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pretrained_models/vocab.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "bwpt = tokenizers.BertWordPieceTokenizer()\n",
    "filepath = \"./datasets/sentence.txt\" # 语料文件\n",
    "#训练分词器\n",
    "bwpt.train(\n",
    "    files=[filepath],\n",
    "    vocab_size=50000, # 这里预设定的词语大小不是很重要\n",
    "    min_frequency=1,\n",
    "    limit_alphabet=1000\n",
    ")\n",
    "# 保存训练后的模型词表\n",
    "bwpt.save_model('./pretrained_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1=[]\n",
    "vocab2=[]\n",
    "with open('../../model/macbert-base-chinese-medical-collation/vocab.txt','r',encoding='utf-8') as f1:\n",
    "    for line in f1:\n",
    "        vocab1.append(line)\n",
    "    f1.close()\n",
    "with open('./pretrained_models/vocab.txt','r',encoding='utf-8') as f2:\n",
    "    for line in f2:\n",
    "        vocab2.append(line)\n",
    "    f2.close()\n",
    "\n",
    "for v in vocab1:\n",
    "    if not v in vocab2:\n",
    "        vocab2.append(v)\n",
    "len(vocab2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pretrained_models/vocab.txt','w',encoding='utf-8') as f2:\n",
    "    for v in vocab2:\n",
    "        f2.write(v)\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt=ts.BertTokenizer(vocab_file='./pretrained_models/vocab.txt')\n",
    "len(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小细胞', '肺癌', '术后', '化疗', '或', '放化疗', '后', '预防性', '脑', '照射', '的', '临床', '观察']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtt=\"小细胞肺癌术后化疗或放化疗后预防性脑照射的临床观察\"\n",
    "sp1=jieba.tokenize(testtt)\n",
    "[w[0] for w in sp1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小细胞', '肺癌', '术后', '化疗', '或', '放化疗', '后', '预防性', '脑', '照射', '的', '临床', '观察']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.load_userdict('./datasets/med_word.txt')\n",
    "sp2=jieba.tokenize(testtt)\n",
    "[w[0] for w in sp2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小细胞', '肺癌', '术后', '化疗', '或', '放化疗', '后', '预防性', '脑', '照射', '的', '临床', '观察']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in jieba.cut(testtt)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
