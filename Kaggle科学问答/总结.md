# 一、数据处理

爬取Wikipedia文档，将每个维基百科页面拆分为段落（按“\n\n”）。

利用few-shot prompt+GPT3生成QA训练数据集。

# 二、稀疏检索

采用BM25算法+最近邻搜索的 HNSW 算法的混合检索算法。

使用ElasticSearch分布式搜索引擎。权重为0.87和0.13。

```
PUT byte-image-index
{
  "mappings": {
    "properties": {
      "byte-image-vector": {
        "type": "dense_vector",
        "element_type": "byte",
        "dims": 2,
        "index": true,
        "similarity": "cosine"
      },
      "title": {
        "type": "text"
      },
      "area": {
        "type": "keyword"
      }
    }
  }
}
```

# 三、密集检索

采用512的大小按段落对Wikipedia的文档进行分块，使用FAISS向量数据库存储。

在每个块中添加当前文档的标题信息，用于消解指代。

使用HF上排名前20的向量化模型BGE。

为了减小存储消耗，对index进行量化。

# 四、模型

使用 QLoRA 进行微调。我们将 bitsandbytes 的 4 位量化和 PEFT 的 LoRAModel 整合到我们的训练管道中。QLoRA 的超参数几乎与 QLoRA 论文中的超参数完全相同，除了由于输入时间长和 GPU 内存限制而不得不降低的批量大小。

修改attention mask改变选项顺序进行5次推断

将每层的量化权重（小于 40GB）注册为数据集，并逐层进行推理。通过将 xformers `memory_efficient_attention` 应用于注意力层，即使在长时间的上下文中，我们也能够保持内存消耗线性，从而在 GPU 内存中留出足够的空间。使用accelerate库来完成这一步骤。

# 五、高质量多路召回

三路召回：

v3：根据 Elasticsearch 搜索时的分数对上下文进行排序。
v5：根据与问句的编辑距离对上下文进行排序。
v7：使用语义搜索对上下文进行排序，使用句子转换器实现。
