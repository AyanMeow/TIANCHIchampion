{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFacePipeline,HuggingFaceHub\n",
    "import transformers as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomLLM import LocalLLM\n",
    "mp='/home/xyy/hfdownloader/aliendao-main/dataroot/models/Qwen/Qwen-7B'\n",
    "local_llm=LocalLLM.from_pretrain(repo_path=mp)\n",
    "ts.AutoModelForCausalLM.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PROMPT_TEMPLATE = \"\"\"\n",
    "用户会提出一个需要你回答的问题，你应该对问题进行理解回答。\n",
    "\n",
    "对于用户的问题，你输出的内容应该是一个一行的字符串，下面是一个例子\n",
    "\n",
    "例子:\n",
    "用户：蒙古国的首都是\n",
    "回答：蒙古国的首都是乌兰巴托（Ulaanbaatar）\n",
    "\n",
    "\n",
    "你的回答格式应该按照下面的内容，请注意“---output”等标记都必须输出，这是我用来提取答案的标记。\n",
    "不要输出中文的逗号，不要输出引号。除了格式示例的内容，你不应该输出其他文字。\n",
    "\n",
    "格式示例：\n",
    "---input\n",
    "用户的问题\n",
    "---output\n",
    "你的答案\n",
    "\n",
    "\n",
    "现在，我们开始作答\n",
    "问题: 中国的首都是？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"---input\\n中国的首都是？\\n---output\\n中国的首都是北京（Beijing）。\"}}]}\n"
     ]
    }
   ],
   "source": [
    "import dashscope\n",
    "from dashscope import Generation\n",
    "from http import HTTPStatus\n",
    "import random\n",
    "dashscope.api_key='sk-02b8b1a9175a4cae9bf9571d517d8b81'\n",
    "messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                {'role': 'user', 'content': _PROMPT_TEMPLATE}]\n",
    "response = Generation.call(\n",
    "    Generation.Models.qwen_max,\n",
    "    #messages=messages,\n",
    "    prompt=_PROMPT_TEMPLATE,\n",
    "    # set the random seed, optional, default to 1234 if not set\n",
    "    seed=random.randint(1, 10000),\n",
    "    result_format='message',  # set the result to be \"message\" format.\n",
    ")\n",
    "if response.status_code == HTTPStatus.OK:\n",
    "    print(response['output'])\n",
    "else:\n",
    "    print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "        response.request_id, response.status_code,\n",
    "        response.code, response.message\n",
    "    ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
