{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers as ts\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['浙江杭州市江干区九堡镇三村村一区'],\n",
       " ['prov',\n",
       "  'prov',\n",
       "  'city',\n",
       "  'city',\n",
       "  'city',\n",
       "  'district',\n",
       "  'district',\n",
       "  'district',\n",
       "  'town',\n",
       "  'town',\n",
       "  'town',\n",
       "  'community',\n",
       "  'community',\n",
       "  'community',\n",
       "  'poi',\n",
       "  'poi'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path,train=True):\n",
    "    if train:\n",
    "        name='train.conll'\n",
    "    else:\n",
    "        name='dev.conll'\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    with open(path+'/'+name) as file:\n",
    "        for line in file:\n",
    "            if line == '' or line == '\\n':\n",
    "                if texts:\n",
    "                    yield{\n",
    "                        'text':texts,\n",
    "                        'label':labels\n",
    "                    }\n",
    "                    texts=[]\n",
    "                    labels=[]\n",
    "            else:\n",
    "                sprilts=line.split()\n",
    "                texts.append(sprilts[0])\n",
    "                labels.append(sprilts[1])\n",
    "        if texts:\n",
    "            yield{\n",
    "                'text':texts,\n",
    "                'label':labels\n",
    "            }\n",
    "    file.close()\n",
    "\n",
    "def get_entities(text,label):\n",
    "    entities=[]\n",
    "    cur_entities={}\n",
    "    for t,l in zip(text,label):\n",
    "        if l[0] in 'BOS' and cur_entities:\n",
    "            entities.append(cur_entities)\n",
    "            cur_entities={}\n",
    "        if l[0] in 'BS':\n",
    "            cur_entities={\n",
    "                'text':t,\n",
    "                'entities':[l[2:]]\n",
    "            }\n",
    "        elif l[0] in 'IE':\n",
    "            cur_entities['text']+=t\n",
    "            cur_entities['entities'].append(l[2:])\n",
    "    if cur_entities:\n",
    "        entities.append(cur_entities)\n",
    "    return entities\n",
    "\n",
    "def makedata(data):\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    for _,d in enumerate(data):\n",
    "        entities=get_entities(d['text'],d['label'])\n",
    "        sentence=''\n",
    "        label=[]\n",
    "        for e in entities :\n",
    "            sentence+=e['text']\n",
    "            label.extend(e['entities'])\n",
    "        sentences.append([sentence])\n",
    "        labels.append(label)\n",
    "    return {'text':sentences,'label':labels}\n",
    "\n",
    "train_data=load_data('./datasets/')\n",
    "train_data=makedata(train_data)\n",
    "val_data=load_data('./datasets/',False)\n",
    "val_data=makedata(val_data)\n",
    "train_data['text'][0],train_data['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'prov',\n",
       " 'city',\n",
       " 'district',\n",
       " 'town',\n",
       " 'community',\n",
       " 'poi',\n",
       " 'road',\n",
       " 'roadno',\n",
       " 'subpoi',\n",
       " 'devzone',\n",
       " 'houseno',\n",
       " 'intersection',\n",
       " 'assist',\n",
       " 'cellno',\n",
       " 'floorno',\n",
       " 'distance',\n",
       " 'village_group']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label2int(labels):\n",
    "    ldict=['O']\n",
    "    for l in labels:\n",
    "        for i in l:\n",
    "            if i not in ldict:\n",
    "                ldict.append(i)\n",
    "    return ldict\n",
    "ldict=label2int(train_data['label']+val_data['label'])\n",
    "ldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addr(Dataset):\n",
    "    \"\"\"docstring for Addr.\"\"\"\n",
    "    def __init__(self,data,tokenizer,ldict):\n",
    "        self.text=[]\n",
    "        for t in data['text']:\n",
    "            self.text.append(list(t[0]))\n",
    "        self.encodings=tokenizer(self.text,is_split_into_words=True,padding=True)\n",
    "        labels=[]\n",
    "        for i,l in enumerate(data['label']):\n",
    "            label=[0,]\n",
    "            for t in l:\n",
    "                label.append(ldict.index(t))\n",
    "            for _ in range(0,len(self.encodings['input_ids'][i])-len(label)):\n",
    "                label.append(0)\n",
    "            labels.append(label)\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx])\n",
    "        labels = torch.LongTensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=ts.AutoTokenizer.from_pretrained('../../models/chinese-roberta-wwm-ext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3851, 3736, 4689, 2123, 3797, 2356, 3851, 3736, 4689, 2123, 3797,\n",
       "         2356, 6969, 2336, 1277, 7674, 1298, 6125, 6887, 1921, 4997, 1298, 6662,\n",
       "          121,  121,  121,  121, 1384, 4384, 4413, 7213, 3805, 1814,  121,  121,\n",
       "         2231, 7824, 5440, 1767, 7674, 1298, 3862, 7623, 1324,  102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([ 0,  1,  1,  1,  2,  2,  2,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,\n",
       "          4,  4,  7,  7,  7,  7,  8,  8,  8,  8,  8,  6,  6,  6,  6,  6, 15, 15,\n",
       "         15,  9,  9,  9,  9,  9,  9,  9,  9,  0])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets=Addr(train_data,tokenizer,ldict)\n",
    "val_datasets=Addr(val_data,tokenizer,ldict)\n",
    "val_datasets[1273]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3308, 7345, 1277, 2207, 1068, 1266, 7027,  121,  121,  121,  118,\n",
       "          121, 1384,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test_data(path):\n",
    "    texts=[]\n",
    "    with open(path+'/final_test.txt') as file:\n",
    "        for line in file:\n",
    "            splits=line.split('\\x01')\n",
    "            texts.append(list(splits[1].rsplit()[0]))\n",
    "        file.close()\n",
    "    return texts\n",
    "\n",
    "class AddrTest(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.text=data\n",
    "        self.encodings=tokenizer(data,is_split_into_words=True,padding=True)\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "test_data=load_test_data('./datasets/')\n",
    "test_datasets=AddrTest(test_data,tokenizer)\n",
    "test_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/chinese-roberta-wwm-ext/ were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../../models/chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classfier=ts.AutoModelForTokenClassification.from_pretrained('../../models/chinese-roberta-wwm-ext/',num_labels=len(ldict))\n",
    "device=torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "classfier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = (labels == preds).sum()/len(labels)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "training_args = ts.TrainingArguments(\n",
    "    output_dir='./results',         # output directory 结果输出地址\n",
    "    num_train_epochs=2,             # total # of training epochs 训练总批次\n",
    "    per_device_train_batch_size=64,  # batch size per device during training 训练批大小\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation 评估批大小\n",
    "    logging_dir='./logs/',    # directory for storing logs 日志存储位置\n",
    "    learning_rate=1e-3,             # 学习率\n",
    "    save_steps=False,               # 不保存检查点\n",
    "    disable_tqdm=True\n",
    ")\n",
    "trainer=ts.Trainer(model=classfier,\n",
    "                   args=training_args,\n",
    "                   train_dataset=train_datasets,\n",
    "                   eval_dataset=val_datasets,\n",
    "                   compute_metrics=compute_metrics,\n",
    "                   tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(training_args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2ef1182e0d4cc8a3c8c053e5923ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\自然语言处理\\TIANCHIchampion\\中文NLP地址要素解析\\bert-base.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/TIANCHIchampion/%E4%B8%AD%E6%96%87NLP%E5%9C%B0%E5%9D%80%E8%A6%81%E7%B4%A0%E8%A7%A3%E6%9E%90/bert-base.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2007\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2005\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m scale_before \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scale_after\n\u001b[0;32m   2006\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2007\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m   2008\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step_was_skipped\n\u001b[0;32m   2010\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run:\n\u001b[0;32m   2011\u001b[0m     \u001b[39m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\accelerate\\optimizer.py:145\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure)\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\work\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:466\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    462\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[0;32m    467\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    468\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea60cffefa14c7b86a1a292052aaf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5310684442520142,\n",
       " 'eval_accuracy': 29.446192893401015,\n",
       " 'eval_runtime': 2.5566,\n",
       " 'eval_samples_per_second': 770.55,\n",
       " 'eval_steps_per_second': 12.125,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3682300976f4bc6adb7a955952742bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=trainer.predict(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.6413693 ,  0.29189742,  0.4956774 ,  0.7139142 ,  0.33139053,\n",
       "        -0.63918626,  1.1995587 ,  0.73807526,  0.46714488, -0.7222633 ,\n",
       "        -0.48162812, -0.28190982, -3.45267   , -2.7361784 , -1.817907  ,\n",
       "        -1.4205768 , -4.5757823 , -3.2565966 ], dtype=float32),\n",
       " 0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pp=preds.predictions[0]\n",
    "pp[7],np.argmax(pp[7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
