{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers as ts\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from simple_bert import RoBERTa_CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['浙江杭州市江干区九堡镇三村村一区'],\n",
       " ['prov',\n",
       "  'prov',\n",
       "  'city',\n",
       "  'city',\n",
       "  'city',\n",
       "  'district',\n",
       "  'district',\n",
       "  'district',\n",
       "  'town',\n",
       "  'town',\n",
       "  'town',\n",
       "  'community',\n",
       "  'community',\n",
       "  'community',\n",
       "  'poi',\n",
       "  'poi'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path,train=True):\n",
    "    if train:\n",
    "        name='train.conll'\n",
    "    else:\n",
    "        name='dev.conll'\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    with open(path+'/'+name) as file:\n",
    "        for line in file:\n",
    "            if line == '' or line == '\\n':\n",
    "                if texts:\n",
    "                    yield{\n",
    "                        'text':texts,\n",
    "                        'label':labels\n",
    "                    }\n",
    "                    texts=[]\n",
    "                    labels=[]\n",
    "            else:\n",
    "                sprilts=line.split()\n",
    "                texts.append(sprilts[0])\n",
    "                labels.append(sprilts[1])\n",
    "        if texts:\n",
    "            yield{\n",
    "                'text':texts,\n",
    "                'label':labels\n",
    "            }\n",
    "    file.close()\n",
    "\n",
    "def get_entities(text,label):\n",
    "    entities=[]\n",
    "    cur_entities={}\n",
    "    for t,l in zip(text,label):\n",
    "        if l[0] in 'BOS' and cur_entities:\n",
    "            entities.append(cur_entities)\n",
    "            cur_entities={}\n",
    "        if l[0] in 'BS':\n",
    "            cur_entities={\n",
    "                'text':t,\n",
    "                'entities':[l[2:]]\n",
    "            }\n",
    "        elif l[0] in 'IE':\n",
    "            cur_entities['text']+=t\n",
    "            cur_entities['entities'].append(l[2:])\n",
    "    if cur_entities:\n",
    "        entities.append(cur_entities)\n",
    "    return entities\n",
    "\n",
    "def makedata(data):\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    for _,d in enumerate(data):\n",
    "        entities=get_entities(d['text'],d['label'])\n",
    "        sentence=''\n",
    "        label=[]\n",
    "        for e in entities :\n",
    "            sentence+=e['text']\n",
    "            label.extend(e['entities'])\n",
    "        sentences.append([sentence])\n",
    "        labels.append(label)\n",
    "    return {'text':sentences,'label':labels}\n",
    "\n",
    "train_data=load_data('./datasets/')\n",
    "train_data=makedata(train_data)\n",
    "val_data=load_data('./datasets/',False)\n",
    "val_data=makedata(val_data)\n",
    "train_data['text'][0],train_data['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'prov',\n",
       " 'city',\n",
       " 'district',\n",
       " 'town',\n",
       " 'community',\n",
       " 'poi',\n",
       " 'road',\n",
       " 'roadno',\n",
       " 'subpoi',\n",
       " 'devzone',\n",
       " 'houseno',\n",
       " 'intersection',\n",
       " 'assist',\n",
       " 'cellno',\n",
       " 'floorno',\n",
       " 'distance',\n",
       " 'village_group']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label2int(labels):\n",
    "    ldict=['O']\n",
    "    for l in labels:\n",
    "        for i in l:\n",
    "            if i not in ldict:\n",
    "                ldict.append(i)\n",
    "    return ldict\n",
    "ldict=label2int(train_data['label']+val_data['label'])\n",
    "ldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addr(Dataset):\n",
    "    \"\"\"docstring for Addr.\"\"\"\n",
    "    def __init__(self,data,tokenizer,ldict):\n",
    "        self.text=[]\n",
    "        for t in data['text']:\n",
    "            self.text.append(list(t[0]))\n",
    "        self.encodings=tokenizer(self.text,is_split_into_words=True,padding=True)\n",
    "        labels=[]\n",
    "        for i,l in enumerate(data['label']):\n",
    "            label=[0,]\n",
    "            for t in l:\n",
    "                label.append(ldict.index(t))\n",
    "            for _ in range(0,len(self.encodings['input_ids'][i])-len(label)):\n",
    "                label.append(0)\n",
    "            labels.append(label)\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx])\n",
    "        labels = torch.LongTensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=ts.AutoTokenizer.from_pretrained('../../models/chinese-roberta-wwm-ext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3851, 3736, 4689, 2123, 3797, 2356, 3851, 3736, 4689, 2123, 3797,\n",
       "         2356, 6969, 2336, 1277, 7674, 1298, 6125, 6887, 1921, 4997, 1298, 6662,\n",
       "          121,  121,  121,  121, 1384, 4384, 4413, 7213, 3805, 1814,  121,  121,\n",
       "         2231, 7824, 5440, 1767, 7674, 1298, 3862, 7623, 1324,  102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([ 0,  1,  1,  1,  2,  2,  2,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,\n",
       "          4,  4,  7,  7,  7,  7,  8,  8,  8,  8,  8,  6,  6,  6,  6,  6, 15, 15,\n",
       "         15,  9,  9,  9,  9,  9,  9,  9,  9,  0])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets=Addr(train_data,tokenizer,ldict)\n",
    "val_datasets=Addr(val_data,tokenizer,ldict)\n",
    "val_datasets[1273]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3308, 7345, 1277, 2207, 1068, 1266, 7027,  121,  121,  121,  118,\n",
       "          121, 1384,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test_data(path):\n",
    "    texts=[]\n",
    "    with open(path+'/final_test.txt') as file:\n",
    "        for line in file:\n",
    "            splits=line.split('\\x01')\n",
    "            texts.append(list(splits[1].rsplit()[0]))\n",
    "        file.close()\n",
    "    return texts\n",
    "\n",
    "class AddrTest(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.text=data\n",
    "        self.encodings=tokenizer(data,is_split_into_words=True,padding=True)\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "test_data=load_test_data('./datasets/')\n",
    "test_datasets=AddrTest(test_data,tokenizer)\n",
    "test_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classfier=ts.AutoModelForTokenClassification.from_pretrained('../../models/chinese-roberta-wwm-ext/',num_labels=len(ldict))\n",
    "classfier=ts.AutoModelForTokenClassification.from_pretrained('../../results2')\n",
    "device=torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "classfier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/chinese-roberta-wwm-ext/ were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../../models/chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "mt=RoBERTa_CRF('../../models/chinese-roberta-wwm-ext/',len(ldict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = (labels == preds).sum()/len(labels)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "training_args = ts.TrainingArguments(\n",
    "    output_dir='./results',         # output directory 结果输出地址\n",
    "    num_train_epochs=2,             # total # of training epochs 训练总批次\n",
    "    per_device_train_batch_size=64,  # batch size per device during training 训练批大小\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation 评估批大小\n",
    "    logging_dir='./logs/',    # directory for storing logs 日志存储位置\n",
    "    learning_rate=1e-3,             # 学习率\n",
    "    save_steps=False,               # 不保存检查点\n",
    "    disable_tqdm=True\n",
    ")\n",
    "trainer=ts.Trainer(model=classfier,\n",
    "                   args=training_args,\n",
    "                   train_dataset=train_datasets,\n",
    "                   eval_dataset=train_datasets,\n",
    "                   compute_metrics=compute_metrics,\n",
    "                   tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(training_args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.477965507656336e-05, 'eval_accuracy': 51.99785456187895, 'eval_runtime': 12.8105, 'eval_samples_per_second': 691.308, 'eval_steps_per_second': 10.85}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.477965507656336e-05,\n",
       " 'eval_accuracy': 51.99785456187895,\n",
       " 'eval_runtime': 12.8105,\n",
       " 'eval_samples_per_second': 691.308,\n",
       " 'eval_steps_per_second': 10.85}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3682300976f4bc6adb7a955952742bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=trainer.predict(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.6413693 ,  0.29189742,  0.4956774 ,  0.7139142 ,  0.33139053,\n",
       "        -0.63918626,  1.1995587 ,  0.73807526,  0.46714488, -0.7222633 ,\n",
       "        -0.48162812, -0.28190982, -3.45267   , -2.7361784 , -1.817907  ,\n",
       "        -1.4205768 , -4.5757823 , -3.2565966 ], dtype=float32),\n",
       " 0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pp=preds.predictions[0]\n",
    "pp[7],np.argmax(pp[7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
