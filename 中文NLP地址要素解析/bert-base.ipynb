{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers as ts\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from simple_bert import RoBERTa_CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['浙江杭州市江干区九堡镇三村村一区'],\n",
       " ['prov',\n",
       "  'prov',\n",
       "  'city',\n",
       "  'city',\n",
       "  'city',\n",
       "  'district',\n",
       "  'district',\n",
       "  'district',\n",
       "  'town',\n",
       "  'town',\n",
       "  'town',\n",
       "  'community',\n",
       "  'community',\n",
       "  'community',\n",
       "  'poi',\n",
       "  'poi'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path,train=True):\n",
    "    if train:\n",
    "        name='train.conll'\n",
    "    else:\n",
    "        name='dev.conll'\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    with open(path+'/'+name) as file:\n",
    "        for line in file:\n",
    "            if line == '' or line == '\\n':\n",
    "                if texts:\n",
    "                    yield{\n",
    "                        'text':texts,\n",
    "                        'label':labels\n",
    "                    }\n",
    "                    texts=[]\n",
    "                    labels=[]\n",
    "            else:\n",
    "                sprilts=line.split()\n",
    "                texts.append(sprilts[0])\n",
    "                labels.append(sprilts[1])\n",
    "        if texts:\n",
    "            yield{\n",
    "                'text':texts,\n",
    "                'label':labels\n",
    "            }\n",
    "    file.close()\n",
    "\n",
    "def get_entities(text,label):\n",
    "    entities=[]\n",
    "    cur_entities={}\n",
    "    for t,l in zip(text,label):\n",
    "        if l[0] in 'BOS' and cur_entities:\n",
    "            entities.append(cur_entities)\n",
    "            cur_entities={}\n",
    "        if l[0] in 'BS':\n",
    "            cur_entities={\n",
    "                'text':t,\n",
    "                'entities':[l[2:]]\n",
    "            }\n",
    "        elif l[0] in 'IE':\n",
    "            cur_entities['text']+=t\n",
    "            cur_entities['entities'].append(l[2:])\n",
    "    if cur_entities:\n",
    "        entities.append(cur_entities)\n",
    "    return entities\n",
    "\n",
    "def makedata(data):\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    for _,d in enumerate(data):\n",
    "        entities=get_entities(d['text'],d['label'])\n",
    "        sentence=''\n",
    "        label=[]\n",
    "        for e in entities :\n",
    "            sentence+=e['text']\n",
    "            label.extend(e['entities'])\n",
    "        sentences.append([sentence])\n",
    "        labels.append(label)\n",
    "    return {'text':sentences,'label':labels}\n",
    "\n",
    "train_data=load_data('./datasets/')\n",
    "train_data=makedata(train_data)\n",
    "val_data=load_data('./datasets/',False)\n",
    "val_data=makedata(val_data)\n",
    "train_data['text'][0],train_data['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'prov',\n",
       " 'city',\n",
       " 'district',\n",
       " 'town',\n",
       " 'community',\n",
       " 'poi',\n",
       " 'road',\n",
       " 'roadno',\n",
       " 'subpoi',\n",
       " 'devzone',\n",
       " 'houseno',\n",
       " 'intersection',\n",
       " 'assist',\n",
       " 'cellno',\n",
       " 'floorno',\n",
       " 'distance',\n",
       " 'village_group']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label2int(labels):\n",
    "    ldict=['O']\n",
    "    for l in labels:\n",
    "        for i in l:\n",
    "            if i not in ldict:\n",
    "                ldict.append(i)\n",
    "    return ldict\n",
    "ldict=label2int(train_data['label']+val_data['label'])\n",
    "ldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addr(Dataset):\n",
    "    \"\"\"docstring for Addr.\"\"\"\n",
    "    def __init__(self,data,tokenizer,ldict):\n",
    "        self.text=[]\n",
    "        for t in data['text']:\n",
    "            self.text.append(list(t[0]))\n",
    "        self.encodings=tokenizer(self.text,is_split_into_words=True,padding=True)\n",
    "        labels=[]\n",
    "        for i,l in enumerate(data['label']):\n",
    "            label=[0,]\n",
    "            for t in l:\n",
    "                label.append(ldict.index(t))\n",
    "            for _ in range(0,len(self.encodings['input_ids'][i])-len(label)):\n",
    "                label.append(0)\n",
    "            labels.append(label)\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx])\n",
    "        labels = torch.LongTensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=ts.AutoTokenizer.from_pretrained('../../model/chinese-roberta-wwm-ext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3851, 3736, 4689, 2123, 3797, 2356, 3851, 3736, 4689, 2123, 3797,\n",
       "         2356, 6969, 2336, 1277, 7674, 1298, 6125, 6887, 1921, 4997, 1298, 6662,\n",
       "          121,  121,  121,  121, 1384, 4384, 4413, 7213, 3805, 1814,  121,  121,\n",
       "         2231, 7824, 5440, 1767, 7674, 1298, 3862, 7623, 1324,  102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([ 0,  1,  1,  1,  2,  2,  2,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,\n",
       "          4,  4,  7,  7,  7,  7,  8,  8,  8,  8,  8,  6,  6,  6,  6,  6, 15, 15,\n",
       "         15,  9,  9,  9,  9,  9,  9,  9,  9,  0])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets=Addr(train_data,tokenizer,ldict)\n",
    "val_datasets=Addr(val_data,tokenizer,ldict)\n",
    "val_datasets[1273]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3308, 7345, 1277, 2207, 1068, 1266, 7027,  121,  121,  121,  118,\n",
       "          121, 1384,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test_data(path):\n",
    "    texts=[]\n",
    "    with open(path+'/final_test.txt') as file:\n",
    "        for line in file:\n",
    "            splits=line.split('\\x01')\n",
    "            texts.append(list(splits[1].rsplit()[0]))\n",
    "        file.close()\n",
    "    return texts\n",
    "\n",
    "class AddrTest(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.text=data\n",
    "        self.encodings=tokenizer(data,is_split_into_words=True,padding=True)\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "test_data=load_test_data('./datasets/')\n",
    "test_datasets=AddrTest(test_data,tokenizer)\n",
    "test_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertcfig=ts.BertConfig.from_pretrained('../../model/chinese-roberta-wwm-ext/')\n",
    "bertcfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert=ts.BertModel.from_pretrained('../../model/chinese-roberta-wwm-ext/',config=bertcfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt=RoBERTa_CRF('../../model/chinese-roberta-wwm-ext/',len(ldict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl=DataLoader(train_datasets,batch_size=2)\n",
    "for step ,batch in enumerate(tdl):\n",
    "    out,loss=mt(batch)\n",
    "    if step > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path):\n",
    "    texts=[]\n",
    "    with open(path+'/final_test.txt') as file:\n",
    "        for line in file:\n",
    "            splits=line.split('\\x01')\n",
    "            texts.append(list(splits[1].rsplit()[0]))\n",
    "        file.close()\n",
    "    return texts\n",
    "\n",
    "class AddrTest(Dataset):\n",
    "    def __init__(self,data,tokenizer,device):\n",
    "        self.text=data\n",
    "        self.encodings=tokenizer(data,is_split_into_words=True,padding=True)\n",
    "        self.device=device\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.LongTensor(self.encodings['input_ids'][idx]).to(self.device)\n",
    "        attention_mask = torch.LongTensor(self.encodings['attention_mask'][idx]).to(self.device)\n",
    "        labels=torch.LongTensor(np.zeros_like(self.encodings['attention_mask'][idx])).to(self.device)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask,'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldict=['O',\n",
    " 'prov',\n",
    " 'city',\n",
    " 'district',\n",
    " 'town',\n",
    " 'community',\n",
    " 'poi',\n",
    " 'road',\n",
    " 'roadno',\n",
    " 'subpoi',\n",
    " 'devzone',\n",
    " 'houseno',\n",
    " 'intersection',\n",
    " 'assist',\n",
    " 'cellno',\n",
    " 'floorno',\n",
    " 'distance',\n",
    " 'village_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=load_test_data('./datasets/')\n",
    "device=torch.device('cuda')\n",
    "tokenizer=ts.AutoTokenizer.from_pretrained('../../model/chinese-roberta-wwm-ext/')\n",
    "test_datasets=AddrTest(test_data,tokenizer,device)\n",
    "test_dl=DataLoader(test_datasets,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../../model/chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=RoBERTa_CRF(bert_path='../../model/chinese-roberta-wwm-ext/',num_labels=len(ldict))\n",
    "state_dict=torch.load('./roberta_crf.pth')\n",
    "model.load_state_dict(state_dict=state_dict)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line(idx,data,preds):\n",
    "    line=str(idx)+'\\u0001'+''.join(data)+'\\u0001'\n",
    "    def split_same(preds):\n",
    "        result = []\n",
    "        current_group = [preds[0]]\n",
    "        for i in range(1, len(preds)):\n",
    "            if preds[i] == preds[i - 1]:\n",
    "                current_group.append(preds[i])\n",
    "            else:\n",
    "                result.append(current_group)\n",
    "                current_group = [preds[i]]\n",
    "        result.append(current_group)\n",
    "        return result\n",
    "    rpreds=split_same(preds)\n",
    "    rpreds=rpreds[1:-1]\n",
    "    bio=''\n",
    "    for s in rpreds:\n",
    "        if s[0] == 0:\n",
    "            bio=bio+(ldict[s[0]]+' ')*(len(s))\n",
    "        elif len(s) == 1:\n",
    "            bio=bio+'B-'+ldict[s[0]]+' '\n",
    "        else:\n",
    "            lens=len(s)-2\n",
    "            bio=bio+'B-'+ldict[s[0]]+' '+('I-'+ldict[s[0]]+' ')*lens+'E-'+ldict[s[0]]+' '\n",
    "    bio=bio.rstrip()\n",
    "    return line+bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\u0001朝阳区小关北里000-0号\u0001B-district I-district E-district B-community I-community I-community E-community B-houseno I-houseno I-houseno I-houseno I-houseno E-houseno\n",
      "1\u0001朝阳区惠新东街00号\u0001B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno E-roadno\n",
      "2\u0001朝阳区南磨房路与西大望路交口东南角\u0001B-district I-district E-district B-road I-road I-road I-road I-road I-road I-road I-road E-road B-intersection E-intersection B-assist I-assist E-assist\n"
     ]
    }
   ],
   "source": [
    "with open('./Akiyan_addr_parsing_runid.txt',\"r\",encoding=\"utf-8\") as file:\n",
    "    for step ,batch in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            out,loss=model(batch)\n",
    "        indexs = batch['input_ids'].tolist()[0].index(102)\n",
    "        sentence = tokenizer.decode(batch['input_ids'][0][1:indexs]).replace(\" \",\"\")\n",
    "        line=create_line(step+1,sentence,out[0])\n",
    "        print(line)\n",
    "        file.write(line+'\\n')\n",
    "        if step >1 :\n",
    "            break\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "with open('./pred.txt',encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        result.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Akiyan_addr_parsing_runid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\x01朝阳区小关北里000-0号\\x01 B-district I-district E-district B-community I-community I-community E-community B-houseno I-houseno I-houseno I-houseno I-houseno E-houseno\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]='1\\x01朝阳区小关北里000-0号\\x01 B-district I-district E-district B-community I-community I-community E-community B-houseno I-houseno I-houseno I-houseno I-houseno E-houseno\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./new.txt','w',encoding='utf-8') as f2:\n",
    "    for line in result:\n",
    "        ss=line.split('\\u0001')\n",
    "        s=ss[2].split()\n",
    "        l=len(ss[1])-len(s)\n",
    "        if l>0:\n",
    "            s=ss[2].rstrip()+' O'*l\n",
    "            s=s[1:]\n",
    "            #print(s)\n",
    "        elif l<0:\n",
    "            print(ss[0])\n",
    "        else:\n",
    "            s=ss[2].rstrip()\n",
    "            s=s[1:]\n",
    "        print(ss[0], ''.join(ss[1]), s, sep='\\u0001', file=f2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
